```{r,eval=FALSE}
<<<<<<< HEAD
install.packages("XML")
install.packages("XML")
=======
library("knitr", lib.loc="~/R/win-library/3.1")
knitr
knit
par(mar = rep(2.3, 4))
for (i in seq(pi/2, -4/3 * pi, length = 12)) {
plot(0, 0, pch = 20, ann = FALSE, axes = FALSE)
arrows(0, 0, cos(i), sin(i))
axis(1, 0, "6"); axis(2, 0, "9")
axis(3, 0, "12"); axis(4, 0, "3"); box()
}
for (i in seq(pi/2, -4/3 * pi, length = 12)) {
plot(0, 0, pch = 20, ann = FALSE, axes = FALSE)
arrows(0, 0, cos(i), sin(i))
axis(1, 0, "6"); axis(2, 0, "9")
axis(3, 0, "12"); axis(4, 0, "3"); box()
}
```{r,fig.width=4,fig.height=4,out.width='0.35\\linewidth',fig.show='animate'}
par(mar = rep(2.3, 4))
for (i in seq(pi/2, -4/3 * pi, length = 12)) {
plot(0, 0, pch = 20, ann = FALSE, axes = FALSE)
arrows(0, 0, cos(i), sin(i))
axis(1, 0, "6"); axis(2, 0, "9")
axis(3, 0, "12"); axis(4, 0, "3"); box()
}
```{r demo, prompt=TRUE}
a=1
```
plot(1)
par(mar = c(4, 4, 4, 4))
plot(1)
library(xlsx)
rm(list=ls())
setwd()
library(RCurl)
library(stringr)
temp=getURL("http://quote.eastmoney.com/stocklist.html",.encoding="GB2312")
getwd()
pattern="http://quote.eastmoney.com/"
library(XML)
detach("package:knitr", unload=TRUE)
library("knitr", lib.loc="~/R/win-library/3.1")
detach("package:knitr", unload=TRUE)
library(RCurl)
library(stringr)
library(XML)
temp=getURL("http://quote.eastmoney.com/stocklist.html",.encoding="GB2312")
pattern="http://quote.eastmoney.com/"
temp1=strsplit(x=temp,split =pattern )
names(temp1)
temp1=strsplit(x=temp,split =pattern )[[1]]
temp1
temp1[[1]]
temp2=temp1[grep(pattern = "</a></li>",  temp1)]
temp3=unlist(strsplit(temp2,"</a></li>"))
temp1=strsplit(x=temp,split =pattern )[[1]]
temp2=temp1[grep(pattern = "</a></li>",  temp1)]
temp3=unlist(strsplit(temp2,"</a></li>"))
tmp3
temp3
table=readHTMLTable(temp)
tmp=readLines(con = temp)
temp=getURL("http://quote.eastmoney.com/stocklist.html",.encoding="GB2312")
tmp=readLines(con = temp)
rm(list=ls())
#获取沪深股票的代码，用的是东方财富网的数据
## 加载包
library(RCurl)
library(stringr)
library(XML)
#网址
url=getURL("http://quote.eastmoney.com/stocklist.html",.encoding="GB2312")
url="http://quote.eastmoney.com/stocklist.html"
rawdat=getURL(url,.encoding="GB2312")
rawdat1=strsplit(x=rawdat,split =pattern )
pattern="http://quote.eastmoney.com/"
rawdat1=strsplit(x=rawdat,split =pattern )[[1]]
rawdat1[[1]]
rawdat1[[2500]]
grep(pattern = "</a></li>",  rawdat1)
grepl(pattern = "</a></li>",  rawdat1)
tmp=rawdat1[grepl(pattern = "</a></li>",  tmp)]
tmp=strsplit(x=rawdat,split =pattern )[[1]]
tmp=rawdat1[grepl(pattern = "</a></li>",  tmp)]
tmp[2500]
tmp=unlist(strsplit(tmp,"</a></li>"))
tmp[1]
tmp[2500]
tmp[2501]
gc()
library("WindR", lib.loc="~/R/win-library/3.1")
w.start()
library("XML", lib.loc="~/R/win-library/3.1")
rm(list=ls())
library(kintr)
opts_chunk$set(comment=NA, fig.width=6, fig.height=6,cache = TRUE)
library(knitr)
opts_chunk$set(comment=NA, fig.width=6, fig.height=6,cache = TRUE)
library(XML)
library(kintr)
library(knitr)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE)
doc <- xmlTreeParse(fileUrl,useInternal=TRUE)
fileUrl <- "http://www.w3school.com.cn/example/xmle/simple.xml"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE)
doc
rootNode <- xmlRoot(doc) #XML节点
names(rootNode)
xmlName(rootNode)
rootNode[[1]]
rootNode[[1]][[1]]
rootNode[[1]][[2]]
rootNOde[[2]]
xpathSApply(rootNode,"/food",xmlValue)  #顶层节点
xpathSApply(rootNode,"//name",xmlValue) #次一层节点
xpathSApply(rootNode,food[@name],xmlValue) #次一层节点
xpathSApply(rootNode,food[@attr-name="name"],xmlValue) #次一层节点
xpathSApply(rootNode,food[name],xmlValue) #次一层节点
xpathSApply(rootNode,"food[name]",xmlValue) #次一层节点
xpathSApply(rootNode,"food[attr-name="name"]",xmlValue) #次一层节点
xpathSApply(rootNode,"food[attr-name='name']",xmlValue) #次一层节点
rm(list=ls())
library(XML)
fileUrl <- "http://quote.eastmoney.com/stocklist.html"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE)
doc <- xmlTreeParse(fileUrl,useInternal=TRUE,encoding = "GBK")
fileUrl <- "http://bbs.10jqka.com.cn/codelist.html"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE,encoding = "GBK")
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
xpathSApply(doc,"//li[@class='score']",xmlValue)
scores <- xpathSApply(doc,"//li[@class='score']",xmlValue)
teams <- xpathSApply(doc,"//li[@class='team-name']",xmlValue)
scores
teams
fileUrl <- "http://www.w3school.com.cn/example/xmle/simple.xml"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE)
xpathSApply(doc,"food[@class]",xmlValue) #次一层节点
xpathSApply(doc,"food[@class="name"]",xmlValue) #对文档操作而不是节点文档，次一层节点
xpathSApply(doc,"//food[@class]",xmlValue) #次一层节点
xpathSApply(doc,"//food[@class='name']",xmlValue)
xpathSApply(doc,"//food[@class='price']",xmlValue) #对文档操作而不是节点文档，次一层节点
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
scores <- xpathSApply(doc,"//li[@class='score']",xmlValue) #对文档操作，而不是节点文件
teams <- xpathSApply(doc,"//li[@class='team-name']",xmlValue)
scores
teams
<<<<<<< HEAD
library(XML)
library(RCurl)
url1="https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
xData <- getURL(url1) #载入数据
url="http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
xData <- getURL(url) #Changing https to http also seems to work.
doc=xmlTreeParse( file = xData,useInternal=TRUE)
fileUrl <- "http://bbs.10jqka.com.cn/codelist.html"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE,encoding = "GBK")
rootnode=xmlRoot(doc)
li=xpathSApply(doc,"//li",xmlValue)
li[[2300]]
li[[2000]]
li
li_vec=unlist(li)
li_vec
read.csv( li_vec,header = F,sep = " ","stockname.csv")
write.csv( li_vec,"stockname.csv",header = F,sep = " ")
write.csv( li_vec,"stockname.csv")
library(stringr)
stri_enc_detect(li)  #检测文档编码
install.packages("stringi")
library(stringi)
enc=stri_enc_detect(li)  #检测文档编码
enc
enc=stri_enc_detect(li_vec)
enc
write.csv( li_vec,"stockname.csv",fileEncoding="GBK")
library(XML)
fileUrl <- "http://bbs.10jqka.com.cn/codelist.html"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE,encoding = "UTF-8")
rootnode=xmlRoot(doc)
li=xpathSApply(doc,"//li",xmlValue) #发现读取回来的文字是乱码
li_vec=unlist(li)
li
doc <- htmlTreeParse(fileUrl,useInternal=TRUE,encoding = "GBK")
iconv(x = doc,from = "GBK",to = "UTF-8")
rm(list=ls())
library(XML)
fileUrl <- "http://bbs.10jqka.com.cn/codelist.html"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE,encoding = "GBK")
iconv(x = doc,from = "GBK",to = "UTF-8")
rootnode=xmlRoot(doc)
li=xpathSApply(doc,"//li",xmlValue) #发现读取回来的文字是乱码
li_vec=unlist(li)
iconv(x = li_vec,from = "GBK",to = "UTF-8")
fileUrl <- "http://bbs.10jqka.com.cn/codelist.html"
doc=getURL(url = fileUrl,.encoding = "GBK")
class(doc)
doc=iconv(x = doc,from = "GBK",to = "UTF-8")
rootnode=xmlRoot(doc) #直接取节点
methods(xmlRoot)
url="http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
xData <- getURL(url) #Changing https to http also seems to work.
class(xData)
doc=xmlTreeParse( file = xData,useInternal=TRUE)
rm(list=ls())
library(XML)
library(RCurl)
url="http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
xData <- getURL(url) #Changing https to http also seems to work.
doc=xmlTreeParse( file = xData,useInternal=TRUE)
doc=xmlTreeParse(file = doc,useInternalNodes = T)
library(XML)
library(RCurl)
fileUrl <- "http://bbs.10jqka.com.cn/codelist.html"
doc=getURL(url = fileUrl,.encoding = "GBK")
class(doc) #发现doc是字符串，可以使用iconv函数转换成UTF-8格式
doc=iconv(x = doc,from = "GBK",to = "UTF-8")
doc=xmlTreeParse(file = doc,useInternalNodes = T)
rm(list=ls())
library(XML)
library(RCurl)
fileUrl <- "http://bbs.10jqka.com.cn/codelist.html"
doc=getURL(url = fileUrl,.encoding = "GBK")
class(doc) #发现doc是字符串，可以使用iconv函数转换成UTF-8格式
doc=iconv(x = doc,from = "GBK",to = "UTF-8")
doc=xmlTreeParse(file = doc,useInternalNodes = T) #报错，标签不匹配等
rootnode=xmlRoot(doc) #直接取节点,因为doc是字符串不可行
install.packages("jsonlite")
rm(list=ls())
library(knitr)
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
class(jsonData)
names(jsonData)
names(jsonData)
names(jsonData$owner)
jsonData$owner$login
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
iris2 <- fromJSON(myjson)
print(iris2)
cat(myjson)[1]
cat(myjson)[[1]]
library(pryr)
library("pryr", lib.loc="~/R/win-library/3.1")
install.packages("Rcpp")
library(pryr)
otype(jsonData)
fattern(jsonData)
flatten(jsonData)
class(flatten(jsonData))
df=flatten(jsonData)
str(df)
which(names(df)=="repos_url")
which(names(jsonData)=="repos_url",arr.ind = T)
names(jsonData)
which(names(df)=="html_url")
which(names(df)=="html_url",arr.ind = T)
which(names(jsonData)=="html_url",arr.ind = T)
names(df)
names(jsonData)
names(jsonData)
names(jsonData$owner)
jsonData["owner",]
jsonData$owner
class(jsonData$owner)
str(jsonData$owner)
clear
cl()
clear()
install.packages("data.tree")
install.packages("data.table")
library(data.table)
df=data.frame(a=sample(c(1:10),size = 20,replace = T),b=sample(letters[1:10],20,T))
#新建dt
dt=data.table(a=sample(c(1:10),size = 20,replace = T),b=sample(letters[1:10],20,T))
dt2=as.data.table(x = df)
head(dt)
dt[3,]
dt[,1]
dt["a"]
methods([)
methods(plot)
methods([])
methods("[")
getS3method(f = "[",class = "data.table")
dt[,list(mean(a),length(b))]
dt[,table(b)]
dt2=dt
head(dt)
dt2[1,1]=30
head(dt) #dt发生变化
dt[1,1]=100
head(dt2)
dt[1,1]
dt2=dt
head(dt)
dt2[1,]=c(30,"x")
head(dt) #dt发生变化
install.packages("quantmod")
install.packages("jpeg")
=======
>>>>>>> refs/remotes/origin/master
>>>>>>> 91b0c31edd76c38c4a7b0f8195e55a4c5bd8215b
ave(1:3)  # no grouping -> grand mean
attach(warpbreaks)
str(warpbreaks)
ave(breaks, wool)
warpbreaks
ave(breaks, tension)
ave(breaks, tension, FUN = function(x) mean(x, trim = 0.1))
ave(breaks, tension,sum)
ave(breaks, sum)
attach(InSectSprays)
detach()
attach(warpbreaks)
ave(breaks, tension)
ave(breaks, tension,mean)
head(warpbreaks)
ave(breaks, tension, FUN = function(x) mean(x, trim = 0.1))
a=warpbreaks
View(a)
ave(a$breaks,a$wool)
tapply(a$breaks,a$wool,mean)
tapply(a$breaks,a$tension,mean)
ave(breaks, tension)
library(datasets)
i=InSectSprays
i=InsectSprays
View(i)
library(plyr)
ddply(i,.(spray),summarize,a=ave(count,fun=sum))
ddply(i,.(spray),summarize,a=ave(count,FUN=sum))
tapply(i$count,i$spray,sum)
ave(i$count,i$spray,FUN=sum)
ave(i$count,i$spray)
ave(i$count,i$spray,sum)
ave(i$count,i$spray,FUN=sum)
(x <- c(sort(sample(1:20, 9)), NA))
(y <- c(sort(sample(3:23, 7)), NA))
intersect(x, y)
install.packages("rvest")
library(rvest)
url = 'http://movie.douban.com/top250?format=text'
score =  html_text(web %>% html_nodes("span") %>% html_nodes("em"))
web = html(url,encoding="UTF-8")
web = read_html(url,encoding="UTF-8")
score =  html_text(web %>% html_nodes("span") %>% html_nodes("em"))
movie <- read_html("http://www.imdb.com/title/tt1490017/")
cast <- html_nodes(movie, "#titleCast span.itemprop")
rm(list=ls())
movie <- read_html("http://www.imdb.com/title/tt1490017/")
movie <- read_html("http://www.imdb.com/title/tt1490017/")
cast <- html_nodes(movie, "#titleCast span.itemprop")
cast[1]
demo(package = "rvest")
url = 'http://movie.douban.com/top250?format=text'
web = read_html(url,encoding="UTF-8")
score =  html_text(web %>% html_nodes("span") %>% html_nodes("em"))
year =  web %>% html_nodes("p")
year =  html_text(year[seq(1,50,2)])
gy = gregexpr('[0-9]{4}',year)
gd = gregexpr("导演",year)
gz = gregexpr("主演",year)
time = sapply(1:length(gy),function(i) substr(year[i],gy[[i]],gy[[i]]+attr(gy[[i]],'match.length')-1))
direct = sapply(1:length(gy),function(i) substr(year[i],gd[[i]] + 4,gz[[i]] - 4))
actor = sapply(1:length(gy),function(i) substr(year[i],gz[[i]] + 4,gy[[i]] - 4))
ga = gregexpr('\n',actor)
actor = sapply(1:length(gy),function(i) substr(actor[i],1,ga[[i]] - 1))
names =   web %>% html_nodes("img") %>% html_attr("alt")
names = iconv(names,"utf-8","gbk")
rates =  html_text(web %>% html_nodes("span"))
rates = rates[grep("人评价",rates)]
rates = gsub("人评价","",rates)
rates
rm(list=ls())
library(rvest)
url = 'http://movie.douban.com/top250?format=text'
web = read_html(url,encoding="UTF-8")
web
score =  html_text(web %>% html_nodes("span") %>% html_nodes("em"))
year =  web %>% html_nodes("p")
year
seq(1,50,2)
year =  html_text(year[seq(1,50,2)])
gy = gregexpr('[0-9]{4}',year)
gd = gregexpr("导演",year)
gz = gregexpr("主演",year)
gd
rm(list=ls())
library(rvest)
lagou<-"http://www.lagou.com/jobs/list_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90?kd=%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90&spc=2&pl=&gj=&xl=&yx=&gx=&st=&labelWords=&lc=&workAddress=&city=%E6%B7%B1%E5%9C%B3&requestId=&pn=3"
web<-html(lagou,encoding="UTF-8") #读取数据，规定编码
web<-read_html(lagou,encoding="UTF-8") #读取数据，规定编码
html_nodes("li div.hot_pos_l a")
movie <- read_html("http://www.imdb.com/title/tt1490017/")
cast <- html_nodes(movie, "#titleCast span.itemprop")
html_text(cast)
position<-web %>% html_nodes("li div.hot_pos_l a") %>% html_text()
list_lagou<-web %>% html_nodes("li.clearfix")
getdata<-function(page,urlwithoutpage){
url=paste0(urlwithoutpage,page) #这里输入拉勾网没有页码的url
web<-html(url,encoding="UTF-8") #读取数据，规定编码,access用
list_lagou<-web %>% html_nodes("li.clearfix") #获得一个清单，15个职位
title<-list_lagou %>% html_nodes("div.hot_pos_l div.mb10 a")%>%html_text()
company<-list_lagou %>% html_nodes("div.hot_pos_r div.mb10 a")%>%html_text()
link<-gsub("\\?source\\=search","",list_lagou %>% html_nodes("div.hot_pos_l div.mb10 a")%>%html_attr("href"))
}
temp<-list_lagou %>% html_nodes("div.hot_pos_l span")
city<-temp[seq(1,90,by=6)] %>% html_text()
salary<-gsub("月薪：","",temp[seq(2,90,by=6)]%>% html_text())
year<-gsub("经验：","",temp[seq(3,90,by=6)]%>% html_text())
degree<-gsub("最低学历：","",temp[seq(4,90,by=6)]%>%html_text())
benefit<-gsub("职位诱惑：","",temp[seq(5,90,by=6)]%>% html_text())
time<-temp[seq(6,90,by=6)]%>%html_text()
data.frame(title,company,city,salary,year,degree,benefit,time,link)
rm(list=ls())
ateam <- read_html("http://www.boxofficemojo.com/movies/?id=ateam.htm")
html_nodes(ateam, "center")
html_nodes(ateam, "center font")
html_nodes(ateam, "center font b")
ateam %>% html_nodes("center") %>% html_nodes("td")
ateam %>% html_nodes("center") %>% html_nodes("font")
ateam %>% html_nodes("center") %>% html_nodes("font") %>% html_nodes("b")
ateam %>% html_nodes("center") %>% html_nodes("td")
td <- ateam %>% html_nodes("center") %>% html_nodes("td")
td %>% html_nodes("font")
td %>% html_node("font")
library(magrittr)
ateam %>% html_nodes("table")
ateam %>% html_nodes("table") %>% extract2(1)
ateam %>% html_nodes("table") %>% extract2(1) %>% html_nodes("img")
ateam %>% html_nodes("table") %>% `[[`(1) %>% html_nodes("img")
ateam %>%
html_nodes(xpath = "//center//font//b") %>%
html_nodes(xpath = "//b")
movie <- read_html("http://www.imdb.com/title/tt1490017/")
cast <- html_nodes(movie, "#titleCast span.itemprop")
html_text(cast)
html_name(cast)
html_attrs(cast)
html_attr(cast, "class")
html_text(cast)
html_name(cast)
html_attrs(cast)
html_attr(cast, "class")
demo()
demo(tripadvisor)
